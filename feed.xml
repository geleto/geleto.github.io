<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Angel Popov&#39;s Blog</title>
  <subtitle></subtitle>
  <link href="https://geleto.github.io/feed.xml" rel="self"/>
  <link href="https://geleto.github.io/"/>
  
    <updated>2025-11-27T00:00:00Z</updated>
  
  <id>https://geleto.github.io</id>
  <author>
    <name>Angel Popov</name>
    <email>geleto@gmail.com</email>
  </author>
  
    
    <entry>
      <title>The Kitchen Chef&#39;s Guide to Taming Chaos</title>
      <link href="https://geleto.github.io/posts/cascada-kitchen-chef/"/>
      <updated>2025-10-19T00:00:00Z</updated>
      <id>https://geleto.github.io/posts/cascada-kitchen-chef/</id>
      <content type="html">
        <![CDATA[
      <p>The empty stares I get when I try to explain what I'm working on‚Ä¶ well, let's just say I've learned to recognize the precise moment someone mentally taps out, and I don't blame them.</p>
<p>So this article is my attempt to explain something complex in a simple, friendly way - no jargon, no tech-speak, just something intuitive and fun to read.</p>
<p>If you can handle the technical version without mentally checking out, here's the full deep-dive: üëâ <a href="https://geleto.github.io/posts/cascada-script-intro/">geleto.github.io/posts/cascada-script-intro/</a></p>
<p>One of the things I'm working on is the <strong><a href="https://github.com/geleto/cascada/blob/master/docs/cascada/script.md">Cascada language</a></strong> - not a general-purpose language for building apps, but a script your app uses to coordinate and automatically run in parallel lots of interconnected tasks.</p>
<p>Modern software isn't just doing one thing at a time anymore - it might clean up raw data, split it into multiple chunks for parallel processing, send each chunk to an AI to extract insights, use the results to create summaries, and then generate a polished final deliverable - all while, at the same time, sending real-time alerts to users and updating dashboards. Run all of that step-by-step, and suddenly you're waiting minutes instead of seconds.. With Cascada You write simple, in-order, sequential logic, and it automatically runs independent tasks in parallel while guaranteeing every step happens exactly when its input data is ready.</p>
<p>Most people understand cooking better than concurrency, so why not explain Cascada using cooks, ingredients, and a world-class restaurant kitchen that keeps everything running in harmony?</p>
<p>Aprons on - let's step into a kitchen during dinner rush and see how this works...</p>
<hr>
<h2>The Problem: The Overwhelmed Chef</h2>
<img src="/images/posts/chef.png" alt="Overwhelmed chef in a busy kitchen" style="float: left; margin-right: 20px; margin-bottom: 10px; max-width: 200px; border-radius: 8px;">
<p>Imagine a kitchen with a talented Head Chef working alone. He's skilled, but he does only one thing at a time - and it's painfully slow. And it gets worse... modern kitchens (and modern applications) aren't preparing single meals - they're handling <strong>dozens of simultaneous orders</strong>.</p>
<p>He is receiving shipments, checking multiple suppliers, cooking  - all while trying to get food out fast.</p>
<p>It's a mental nightmare. What you really need is a system that can handle massive parallelism while automatically understanding what must wait for what.</p>
<hr>
<h2>The Naive Fix: Hire More Cooks</h2>
<p>So the Chef tries the obvious solution: hire more cooks and divide the work.</p>
<p>An order arrives for a full five-course banquet for twelve guests. The chef assigns each course to a different cook: &quot;Everyone start now - we need this done at the same time!&quot; All five cooks rush to begin simultaneously. It should be beautifully parallel.</p>
<p>Except:</p>
<p>The truffle oil hasn't arrived yet, but Cook A is already frantically searching for it to start the risotto. Cook B fires up twelve steaks at once - three medium-rare, six medium, three well-done. The first few finish perfectly seared, then sit... cooling on the counter, losing their heat and their magic... while he waits for the last well-done steaks to catch up. And because everyone started at the exact same moment, all five cooks need the oven at precisely 6:15pm.</p>
<p>They stand there, waiting their turn. The oven becomes a bottleneck.</p>
<p>Meanwhile, the salad cook finishes in 8 minutes but sits idle for the next 30 while waiting for the entr√©es. The dessert cook started making souffl√©s immediately... which collapse by the time the main course is finally ready.</p>
<p>The problem? <strong>Dividing the work</strong> seems parallel, but it ignores reality - what the kitchen needs isn't just dividing work among cooks - it needs <strong>intelligent orchestration</strong> that starts each task exactly when its ingredients are ready and naturally spreads all available work.</p>
<hr>
<h2>The Solution: The Cascada Kitchen Manager</h2>
<img src="/images/posts/robot.png" alt="Hard-working robot manager in a busy kitchen" style="float: left; margin-right: 20px; margin-bottom: 10px; max-width: 200px; border-radius: 8px;">
<p>Now, imagine a completely different kitchen. Here, the Head Chef's job is not to manage, but to <strong>design</strong>.</p>
<p>He sits down in a quiet office and writes out the entire, complex banquet recipe. And here is the most crucial part: <strong>he writes it just as he always has, in a simple, sequential, top-to-bottom list of steps. It looks and feels like any other recipe he's ever written.</strong></p>
<p>But this simple-looking recipe is incredibly powerful because <strong>it's a smart, adaptable plan</strong>. The Chef can include rules, choices, and logic as part of his normal, sequential writing:</p>
<ul>
<li>&quot;<strong>If</strong> the season is Winter, use the root vegetables. <strong>Otherwise</strong>, use the summer squash.&quot;</li>
<li>&quot;<strong>For each</strong> of the 12 dinner orders for the main party, prepare one beef medallion to the customer's requested temperature.&quot;</li>
</ul>
<p>The recipe isn't a rigid checklist - it's a flexible blueprint that responds to real-world conditions.</p>
<p>Once this smart, yet familiar-looking recipe is written, his main job is done. He hands it to a brilliant <strong>Kitchen Manager</strong> (that's <a href="https://github.com/geleto/cascada/blob/master/docs/cascada/script.md">Cascada</a>) and can go relax.</p>
<p>The Manager doesn't just read a fixed list. He <strong>interprets this smart recipe in real-time, as things are happening.</strong> He checks the calendar, sees the 12 orders, understands the client's needs, and then executes the Chef's simple, sequential logic with maximum parallelism, handling all the complexity himself.</p>
<hr>
<h2>Understanding Ingredient Dependencies</h2>
<img src="/images/posts/dependencies.png" alt="Ingredient dependencies chart" style="float: left; margin-right: 20px; margin-bottom: 10px; max-width: 200px; border-radius: 8px;">
<p>The Kitchen Manager is smart enough to handle dependencies within this dynamic logic automatically. Imagine the recipe calls for a final pan sauce, which requires three things: the pan the beef was seared in, some saut√©ed shallots, and the reduced red wine.</p>
<p>He orchestrates a multi-part ballet:</p>
<ul>
<li>He has Assistant D searing the beef.</li>
<li>Simultaneously, he has Assistant E saut√©ing shallots.</li>
<li>And at the same time, Assistant F is reducing the wine.</li>
<li>He then tells a final assistant, Assistant G: &quot;Stand by. You cannot start your job until the beef is done, the shallots are ready, AND the wine is reduced. The moment all three are finished - no matter the timing - you will combine them to create the final sauce.&quot;</li>
</ul>
<p>While Assistant G is waiting for these multiple inputs, the rest of the kitchen is not waiting with him! Other dishes are being prepped, other sauces are being reduced. This automatic waiting - without blocking anything else - is the fundamental principle that makes the system so efficient.</p>
<p>The Chef never wrote &quot;wait for the beef&quot; or &quot;check if the shallots are ready.&quot; He just wrote the recipe in order. The Manager figured out the dependencies automatically.</p>
<hr>
<h2>When Order Matters (!)</h2>
<p>In our kitchen, dozens of things happen at once - rice simmering, sauces reducing, plates going out.
But sometimes, one particular part of a dish must follow a very specific sequence.
That's what the <code>!</code> mark is for.</p>
<p>It tells the Kitchen Manager:
<strong>&quot;For this dish or this component, follow these steps in order - no reordering, no overlap - while everything else runs freely in parallel.&quot;</strong></p>
<p>For example:</p>
<ul>
<li><code>Risotto! Toast the rice in olive oil until translucent.</code></li>
<li><code>Risotto! Add a ladle of stock and stir until absorbed.</code></li>
<li><code>Risotto! Repeat until the rice is just al dente.</code></li>
<li>(Meanwhile, other cooks are grilling steaks, prepping salads, and making desserts...)</li>
<li><code>Risotto.sauce! Reduce the white wine base.</code></li>
<li><code>Risotto.sauce! Whisk in butter, then finish with grated parmesan.</code></li>
<li><code>Risotto! Combine the sauce and rice, season, and plate.</code></li>
</ul>
<p>Whenever the Manager sees a <code>!</code> on <strong>risotto</strong>, they assign one cook to perform those risotto steps in the exact written order.
When they see <code>risotto.sauce!</code>, another cook handles those sauce steps sequentially - <strong>only</strong> the sauce follows that order, not the entire dish.</p>
<p>Think of <code>Risotto!</code> and <code>Risotto.sauce!</code> as two separate assembly lines - each runs its own steps in order, but the two lines operate independently and in parallel with each other. Everything else in the kitchen continues running independently as well.</p>
<p>The <code>!</code> doesn't make the kitchen slower - it simply ensures that for that component, <strong>each step happens exactly in sequence</strong>, while the rest of the kitchen stays fully concurrent.</p>
<p>The final combining step automatically waits for both the rice sequence and the sauce sequence to complete - that's the Manager's automatic dependency tracking at work.</p>
<p><em>(And if the recipe ever went deeper, say <code>risotto.sauce.topping!</code>, the same idea would apply - each level keeps its own internal order without blocking anything else.)</em></p>
<hr>
<h2>The Final Pass: Plating Station (@)</h2>
<img src="/images/posts/plate.png" alt="Finished plate" style="float: left; margin-right: 20px; margin-bottom: 10px; max-width: 200px; border-radius: 8px;">
<p>Here's a challenge: In our chaotic parallel kitchen, dishes finish in unpredictable order. The beef for Table 5 might be done before Table 3's. The garnish might arrive before the protein. How do you ensure every plate that leaves the kitchen is perfect and consistently presented?</p>
<p>The Chef solves this with the @ symbol, which marks items for the <strong>final pass plating station</strong> - a dedicated area where one master plater assembles each dish only after ALL its components are ready.</p>
<p>Throughout the recipe's logic, you might see lines like this:</p>
<ul>
<li>Beef Medallion @ Place in the center of the plate.</li>
<li>(Logic to determine which vegetable to cook based on season...)</li>
<li>Vegetable @ Arrange artfully next to the medallion.</li>
<li>(More parallel cooking of sauces, sides...)</li>
<li>Pan Sauce @ Drizzle over the medallion.</li>
</ul>
<p>As the Manager interprets the recipe's dynamic logic - perhaps preparing 12 plates because there are 12 guests - he collects every @ instruction he encounters and puts it on a special clipboard. During execution, he notes each @ instruction on his clipboard as he encounters it in the recipe.</p>
<p><strong>Only after every single cooking and preparation step in the entire recipe is complete</strong> does the Manager hand that clipboard to the final plating station. The plater then reads the @ instructions one-by-one, in the exact order they were collected, and assembles each perfect plate.</p>
<p>This ensures that even though the kitchen is running 50 orders simultaneously with components finishing in chaotic order, every dish that goes out is assembled with perfect consistency and care.</p>
<hr>
<h2>The Domino Effect: Handling Kitchen Disasters</h2>
<p>A perfect recipe assumes a perfect kitchen, but in reality, things go wrong. An oven breaks. A key ingredient is missing. An assistant calls in sick.</p>
<p>Traditional kitchens panic - the whole operation can grind to a halt. But the <a href="https://github.com/geleto/cascada/blob/master/docs/cascada/script.md">Cascada</a> Manager handles disasters with what's called <strong><code>error propagation</code></strong> - a controlled ripple effect that contains problems without stopping the entire kitchen.</p>
<p>Imagine someone discovers the truffles that arrived this morning are actually expired and can't be used. The Manager immediately marks <strong>Truffles</strong> as &quot;contaminated&quot; - or in <a href="https://github.com/geleto/cascada/blob/master/docs/cascada/script.md">Cascada</a> terms, &quot;poisoned&quot; (we use this word figuratively to describe how one bad piece of data makes everything depending on it unusable too). This contamination then flows through the dependency chain: the <strong>Truffle Risotto</strong>, which needs those truffles, automatically becomes &quot;unavailable&quot; as well.</p>
<p>Crucially, the assistants making the steak and salad are unaffected. They never needed truffles, so they continue working at full speed. The problem is perfectly contained.</p>
<h3>Three Ways to Handle Disasters</h3>
<p>The Head Chef has powerful options for dealing with unavailable items:</p>
<p><strong>1. Intervene Immediately:</strong> The moment the truffles become unavailable, a contingency plan can trigger automatically: <em>&quot;Send an assistant to the specialty market to buy more!&quot;</em> If they succeed, the problem is solved and the risotto is back on track. This is perfect for retrying failed operations.</p>
<p><strong>2. Substitute Gracefully:</strong> The recipe itself can have a backup plan built in: <em>&quot;If fresh truffles are unavailable, use high-quality mushroom duxelles instead.&quot;</em> The Manager sees this, makes the substitution, and the problem is stopped before it spreads.</p>
<p><strong>3. Check and Adapt Later:</strong> You can also do nothing immediately. The risotto simply remains &quot;unavailable.&quot; Later, you can check its status and decide how to proceed. Maybe you offer the customer an alternative dish. Or perhaps the truffles finally arrive from a backup supplier - in which case you can restart the risotto preparation with the fresh ingredients. The choice of if, when and how to handle the problem is always yours.</p>
<p>The beauty is that unavailability doesn't cause crashes or confusion - it's just information that flows through the system, letting you respond at the right moment.</p>
<hr>
<h2>The Takeaway: Effortless Parallelism</h2>
<p>You write as if every step were a single, simple action. Yet beneath the surface, the system runs a perfectly choreographed symphony of parallel work.</p>
<p>That's the quiet power of being <em>implicitly parallel by design.</em></p>
<h2>Learn More and Try It Yourself</h2>
<p>üöÄ <a href="https://geleto.github.io/posts/cascada-script-intro/">Cascada Script Introduction</a> - A more technical introduction to Cascada Script's syntax, features, and how it solves real async programming challenges</p>
<p>üìñ <a href="https://github.com/geleto/cascada/blob/master/docs/cascada/script.md">Cascada Script Documentation</a></p>
<p>üß© <a href="https://github.com/geleto/cascada">Cascada GitHub Repository</a></p>
<p>ü§ñ <a href="https://github.com/geleto/casai">Casai</a> - an AI orchestration library built on Cascada for effortless, parallel agent and workflow design.</p>
<p>‚ö° <strong>Try it now:</strong></p>
<pre class="language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> cascada-engine</code></pre>
<hr>

    ]]>
      </content>
    </entry>
  
    
    <entry>
      <title>Cascada Script: Parallel by Default, Sequential by Exception</title>
      <link href="https://geleto.github.io/posts/cascada-script-intro/"/>
      <updated>2025-10-30T00:00:00Z</updated>
      <id>https://geleto.github.io/posts/cascada-script-intro/</id>
      <content type="html">
        <![CDATA[
      <p><strong><a href="https://github.com/geleto/cascada/blob/master/docs/cascada/script.md">Cascada Script</a></strong> is a scripting language with JavaScript and TypeScript integration that turns async programming upside down: everything runs in parallel by default, you never write <code>await</code>, and concurrency issues like race conditions are no longer a problem - by design.</p>
<h3>The Nightmare: Why Async Programming Is So Hard</h3>
<p><code>async/await</code> is a huge improvement over callback hell. But the moment you want performance, you're back to being a concurrency expert.</p>
<p><strong>The fundamental problem: you manually orchestrate what runs when.</strong> Which operations can run together? Which must wait? You figure it out yourself, every time.</p>
<h4><strong>Shared State</strong></h4>
<p>All operations read and write to shared variables or objects. The complexity explodes:</p>
<ul>
<li>
<p><strong>Manual dependency tracking.</strong> Should operation A <code>await</code> operation B before updating state? Which operations can safely run in parallel? You trace through every possible execution order in your head.</p>
</li>
<li>
<p><strong>Get the order wrong: race conditions.</strong> Operation A reads a value, operation B modifies it, operation A writes based on the stale value. Your state is corrupted.</p>
</li>
<li>
<p><strong>Over-serialize to be safe?</strong> Make everything <code>await</code> everything else to avoid conflicts. Now your parallel code runs sequentially anyway.</p>
</li>
<li>
<p><strong>Changes ripple everywhere.</strong> Add one new operation that touches shared state? Review every other operation to ensure no new conflicts. Your code becomes brittle and impossible to refactor.</p>
</li>
</ul>
<h4><strong>State Machines &amp; Batch Synchronization</strong></h4>
<p>To avoid this complexity, most frameworks use state machines (with chokepoints between states) or batch synchronization (parallel tasks that must all complete). Simpler to reason about, but: 19 API calls finish in 50ms, one takes 3 seconds. Everything waits at the chokepoint for the slowest operation‚Äîand you still manually coordinate the convergence.</p>
<p><strong>The payoff for getting this right is enormous</strong>‚Äîoptimal orchestration can cut response times in half or more. But the manual complexity? That's where code breaks.</p>
<p>What if you could skip the manual orchestration entirely? That's exactly what <strong><a href="https://github.com/geleto/cascada/blob/master/docs/cascada/script.md">Cascada Script</a></strong> does.</p>
<p>Let's dive into how it flips the script on async programming.</p>
<hr>
<h2>1. ‚ö° Parallel by Default</h2>
<p>The most fundamental shift in Cascada is that it's <strong>parallel by default</strong>. In most languages, code runs line by line, one after the other. In Cascada, any independent lines of code run at the same time.</p>
<p>Think about fetching a user's profile and their recent posts. These are two separate operations that don't depend on each other. In traditional JavaScript, you'd need <code>Promise.all</code> to run them concurrently. Cascada does it automatically.</p>
<pre class="language-javascript"><code class="language-javascript"><span class="token comment">// These three operations start at the same time, automatically!</span>
<span class="token keyword">var</span> user <span class="token operator">=</span> <span class="token function">fetchUser</span><span class="token punctuation">(</span><span class="token number">123</span><span class="token punctuation">)</span>
<span class="token keyword">var</span> preferences <span class="token operator">=</span> <span class="token function">getUserPreferences</span><span class="token punctuation">(</span><span class="token number">123</span><span class="token punctuation">)</span>
<span class="token keyword">var</span> analytics <span class="token operator">=</span> <span class="token function">getAnalytics</span><span class="token punctuation">(</span><span class="token number">123</span><span class="token punctuation">)</span>

<span class="token comment">// Build your result with the @ operator</span>
@data<span class="token punctuation">.</span>user <span class="token operator">=</span> user
@data<span class="token punctuation">.</span>preferences <span class="token operator">=</span> preferences
@data<span class="token punctuation">.</span>analytics <span class="token operator">=</span> analytics
<span class="token comment">// Result: { user: {...}, preferences: {...}, analytics: {...} }</span></code></pre>
<p>No <code>Promise.all</code>, no await, no special syntax. If it <em>can</em> run in parallel, it <em>will</em>.</p>
<hr>
<h2>2. üö¶ Data-Driven Flow: Code Runs When Its Inputs Are Ready</h2>
<p>You might be thinking, &quot;What if one operation <em>does</em> depend on another?&quot; Cascada has you covered.</p>
<p>The engine automatically analyzes the data dependencies in your script. An operation will only run once all the variables it needs are ready. This simple rule guarantees the correct order of execution and <strong>completely eliminates race conditions by design</strong>.</p>
<pre class="language-javascript"><code class="language-javascript"><span class="token comment">// 1. This runs first</span>
<span class="token keyword">var</span> user <span class="token operator">=</span> <span class="token function">fetchUser</span><span class="token punctuation">(</span><span class="token number">123</span><span class="token punctuation">)</span>

<span class="token comment">// 2. This depends on 'user', so Cascada waits for it to resolve</span>
<span class="token keyword">var</span> userGreeting <span class="token operator">=</span> <span class="token string">"Hello, "</span> <span class="token operator">+</span> user<span class="token punctuation">.</span>name

<span class="token comment">// 3. Meanwhile, these run in parallel with everything above</span>
<span class="token keyword">var</span> posts <span class="token operator">=</span> <span class="token function">fetchPosts</span><span class="token punctuation">(</span><span class="token number">123</span><span class="token punctuation">)</span>
<span class="token keyword">var</span> comments <span class="token operator">=</span> <span class="token function">fetchComments</span><span class="token punctuation">(</span><span class="token number">123</span><span class="token punctuation">)</span></code></pre>
<p>Here, <code>userGreeting</code> won't be calculated until <code>fetchUser(123)</code> is complete and the <code>user</code> variable has a value. But <code>posts</code> and <code>comments</code> start fetching immediately since they don't depend on <code>user</code>.</p>
<p>No more subtle timing bugs that only appear in production. The engine orchestrates everything automatically.</p>
<hr>
<h2>3. ‚ú® Implicit Concurrency: Write Business Logic, Not Async Plumbing</h2>
<p>This is where the magic really happens. Notice the lack of <code>await</code> in the examples above? In Cascada, you never have to think about whether a variable holds a value or a promise. You just use it.</p>
<pre class="language-javascript"><code class="language-javascript"><span class="token comment">// Traditional JavaScript: Promise hell</span>
<span class="token keyword">const</span> userPromise <span class="token operator">=</span> <span class="token function">fetchUser</span><span class="token punctuation">(</span><span class="token number">123</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token keyword">const</span> name <span class="token operator">=</span> <span class="token keyword">await</span> userPromise<span class="token punctuation">.</span>name<span class="token punctuation">;</span> <span class="token comment">// Wait, can't do this!</span>
<span class="token keyword">const</span> actualUser <span class="token operator">=</span> <span class="token keyword">await</span> userPromise<span class="token punctuation">;</span>
<span class="token keyword">const</span> name <span class="token operator">=</span> actualUser<span class="token punctuation">.</span>name<span class="token punctuation">;</span> <span class="token comment">// Finally!</span></code></pre>
<p>Cascada makes this completely invisible:</p>
<pre class="language-javascript"><code class="language-javascript"><span class="token comment">// Cascada: Just use it</span>
<span class="token keyword">var</span> user <span class="token operator">=</span> <span class="token function">fetchUser</span><span class="token punctuation">(</span><span class="token number">123</span><span class="token punctuation">)</span>

<span class="token comment">// The @data command builds our final JSON output</span>
@data<span class="token punctuation">.</span>greet <span class="token operator">=</span> <span class="token string">"Hello, "</span> <span class="token operator">+</span> user<span class="token punctuation">.</span>name
@data<span class="token punctuation">.</span>email <span class="token operator">=</span> user<span class="token punctuation">.</span>email
@data<span class="token punctuation">.</span>status <span class="token operator">=</span> user<span class="token punctuation">.</span>isActive <span class="token operator">?</span> <span class="token string">"active"</span> <span class="token operator">:</span> <span class="token string">"inactive"</span></code></pre>
<p>Forget <code>.then()</code> and forget manually tracking promises. Cascada handles the asynchronous state invisibly under the hood. You can pass a &quot;future value&quot; (a promise) into a function or use it in an expression, and it just works.</p>
<p>This lets you focus entirely on your business logic, not the async plumbing.</p>
<hr>
<h2>4. ‚û°Ô∏è Implicitly Parallel, Explicitly Sequential</h2>
<p>Of course, sometimes you absolutely need things to happen in a specific order, especially when dealing with operations that have side effects - like writing to a database or making stateful API calls.</p>
<p>For these cases, Cascada provides a simple escape hatch: the <code>!</code> marker. Marking a call or path with <code>!</code> enforces a strict sequential order on that specific path, without slowing down the rest of your script. A call does not need a data dependency from another to run in sequence.</p>
<pre class="language-javascript"><code class="language-javascript"><span class="token comment">// The ! marker creates a sequential chain for a specific path</span>
<span class="token keyword">var</span> account <span class="token operator">=</span> <span class="token function">getBankAccount</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment">// 1. This MUST finish first</span>
account<span class="token operator">!</span><span class="token punctuation">.</span><span class="token function">deposit</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span>
<span class="token comment">// 2. This waits for deposit to complete</span>
account<span class="token punctuation">.</span><span class="token function">getStatus</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment">// 3. This waits for getStatus</span>
account<span class="token operator">!</span><span class="token punctuation">.</span><span class="token function">withdraw</span><span class="token punctuation">(</span><span class="token number">50</span><span class="token punctuation">)</span>

<span class="token comment">// Meanwhile, these run in parallel with everything above</span>
<span class="token keyword">var</span> preferences <span class="token operator">=</span> <span class="token function">getUserPreferences</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
preferences<span class="token operator">!</span><span class="token punctuation">.</span><span class="token function">update</span><span class="token punctuation">(</span>prefs<span class="token punctuation">)</span>   <span class="token comment">// Sequential chain for preferences too</span>
<span class="token keyword">var</span> analytics <span class="token operator">=</span> <span class="token function">fetchAnalytics</span><span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>
<p>The <code>!</code> creates a sequential chain for just that specific path, without affecting the parallelism of everything else. It's <strong>parallel by default, sequential by exception</strong> - the opposite of traditional programming.</p>
<hr>
<h2>5. üìã Chaotic Execution, Predictable Output</h2>
<p>While independent operations run in parallel and can finish in any order, Cascada guarantees that your <strong>final output is assembled predictably</strong>.</p>
<p>Data manipulation commands (like adding an item to a list) are applied in the exact order they appear in your script. So even if a <code>for</code> loop's iterations complete out of order, the final array will be structured correctly.</p>
<pre class="language-javascript"><code class="language-javascript"><span class="token keyword">var</span> userIds <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">,</span> <span class="token number">103</span><span class="token punctuation">]</span>

<span class="token comment">// All three fetchUserDetails calls run in parallel</span>
<span class="token comment">// Maybe user 103's data comes back first</span>
<span class="token keyword">for</span> id <span class="token keyword">in</span> userIds
  <span class="token keyword">var</span> details <span class="token operator">=</span> <span class="token function">fetchUserDetails</span><span class="token punctuation">(</span>id<span class="token punctuation">)</span>
  @data<span class="token punctuation">.</span>users<span class="token punctuation">.</span><span class="token function">push</span><span class="token punctuation">(</span>details<span class="token punctuation">.</span>name<span class="token punctuation">)</span>
endfor

<span class="token comment">// Even so, the final output is ALWAYS predictable and in order:</span>
<span class="token comment">// { "users": ["Alice", "Bob", "Charlie"] }</span></code></pre>
<p>This gives you the best of both worlds: maximum I/O performance from parallel execution, with the reliability of sequential data assembly.</p>
<hr>
<h2>6. ‚ò£Ô∏è Dataflow Poisoning: Resilient Error Handling</h2>
<p>Traditional <code>try/catch</code> blocks don't work well in a massively parallel system. If one of fifty concurrent API calls fails, should everything stop?</p>
<p>Cascada uses a more resilient model called <strong>dataflow poisoning</strong>. When an operation fails, it doesn't throw an exception; it produces an <code>Error Value</code>. This error then &quot;poisons&quot; any other variable or operation that depends on it. Crucially, unrelated operations continue running completely unaffected.</p>
<pre class="language-javascript"><code class="language-javascript"><span class="token comment">// Let's pretend fetchPosts() fails, but fetchUser() succeeds</span>
<span class="token keyword">var</span> user <span class="token operator">=</span> <span class="token function">fetchUser</span><span class="token punctuation">(</span><span class="token number">123</span><span class="token punctuation">)</span>            <span class="token comment">// ‚úÖ Succeeds</span>
<span class="token keyword">var</span> posts <span class="token operator">=</span> <span class="token function">fetchPosts</span><span class="token punctuation">(</span><span class="token number">123</span><span class="token punctuation">)</span>          <span class="token comment">// ‚ùå Fails and becomes an Error Value</span>
<span class="token keyword">var</span> comments <span class="token operator">=</span> <span class="token function">fetchComments</span><span class="token punctuation">(</span>posts<span class="token punctuation">)</span>  <span class="token comment">// ‚ò£Ô∏è Poisoned because it uses 'posts'</span>

<span class="token comment">// Now, let's see what happens:</span>
@data<span class="token punctuation">.</span>userName <span class="token operator">=</span> user<span class="token punctuation">.</span>name     <span class="token comment">// ‚úÖ Works fine, uses the successful result</span>
@data<span class="token punctuation">.</span>postCount <span class="token operator">=</span> posts<span class="token punctuation">.</span>length <span class="token comment">// ‚ùå Becomes an error because 'posts' is poisoned</span>

<span class="token comment">// You can check for errors and provide fallbacks</span>
<span class="token keyword">if</span> posts is error
  posts <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment">// Assign a default value</span>
endif

@data<span class="token punctuation">.</span>postCount <span class="token operator">=</span> posts<span class="token punctuation">.</span>length <span class="token comment">// ‚úÖ Now works with our fallback</span></code></pre>
<p>This approach isolates failures, prevents corrupted data from producing incorrect results, and makes your workflows incredibly robust.</p>
<p>The beauty of dataflow poisoning is that you have <strong>complete control over recovery</strong>. You can detect errors at any point in your workflow (using <code>is error</code> conditions), repair them with fallback values, log them for monitoring, or even retry failed operations - all while the rest of your script continues executing normally.</p>
<hr>
<h2>7. üí° Clean, Expressive Syntax</h2>
<p>Cascada offers a clean, expressive syntax that will feel instantly familiar if you know JavaScript. You can use variables, loops, conditionals, and build reusable macros.</p>
<pre class="language-javascript"><code class="language-javascript"><span class="token comment">// Variables, conditionals, loops</span>
<span class="token keyword">var</span> discount <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>

<span class="token keyword">if</span> userType <span class="token operator">==</span> <span class="token string">"premium"</span>
  <span class="token keyword">var</span> discount <span class="token operator">=</span> <span class="token number">0.10</span>
endif

<span class="token comment">// Build reusable macros</span>
macro <span class="token function">formatPrice</span><span class="token punctuation">(</span>amount<span class="token punctuation">,</span> discount<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
  <span class="token keyword">var</span> final <span class="token operator">=</span> amount <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> discount<span class="token punctuation">)</span>
  @text <span class="token operator">&lt;&lt;</span> <span class="token string">"$"</span> <span class="token operator">+</span> final
endmacro

<span class="token function">formatPrice</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> discount<span class="token punctuation">)</span>  <span class="token comment">// Outputs: $90</span></code></pre>
<p>You get variables (<code>var</code>), conditionals (<code>if/else</code>), loops (<code>for/while</code>), macros for reusability (<code>macro</code>), and modular code organization with <code>import</code> and <code>extends</code>.</p>
<hr>
<h2>8. ‚öôÔ∏è Under the Hood: Chaos Managed Gracefully</h2>
<p>Underneath all this simplicity is a powerful engine that:</p>
<ul>
<li><strong>Tracks data dependencies automatically</strong> so operations run when their inputs are ready</li>
<li><strong>Handles concurrent execution safely</strong> with maximum I/O throughput</li>
<li><strong>Guarantees consistent, deterministic outputs</strong> even with chaotic parallel execution</li>
<li><strong>Propagates errors without crashing</strong> so failures are isolated and manageable</li>
</ul>
<p>You focus on <em>what</em> you want to happen. The engine handles <em>how</em> to make it happen safely and efficiently.</p>
<hr>
<h2>üéØ Why This Matters</h2>
<p>Cascada isn't trying to replace JavaScript - it's designed to be the backbone of your data layer.</p>
<p>Use it to compose complex workflows that wire together LLMs, APIs, databases, and external services. By inverting the traditional programming model - parallel by default, sequential by exception - it lets you build high-performance data pipelines that are surprisingly simple and intuitive all with maximum I/O throughput and minimum mental overhead.</p>
<p>The result? <strong>Code that reads like synchronous logic but executes with the performance of carefully orchestrated async operations.</strong> You get to focus on <em>what</em> you're building instead of <em>how</em> to manage promises, race conditions, and execution order.</p>
<p>And the best part? When you look at your Cascada script six months later, you'll actually understand what it does.</p>
<hr>
<h2>üöÄ Ready to Simplify Your Async Code?</h2>
<p>Cascada is <code>a work in progress</code> under active development and evolving quickly. Install it with:</p>
<pre class="language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> cascada-engine</code></pre>
<p>And start writing async code that makes sense:</p>
<pre class="language-javascript"><code class="language-javascript"><span class="token keyword">import</span> <span class="token punctuation">{</span> AsyncEnvironment <span class="token punctuation">}</span> <span class="token keyword">from</span> <span class="token string">'cascada-engine'</span><span class="token punctuation">;</span>

<span class="token keyword">const</span> env <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">AsyncEnvironment</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token keyword">const</span> script <span class="token operator">=</span> <span class="token template-string"><span class="token template-punctuation string">`</span><span class="token string">
  var user = fetchUser(123)
  var posts = fetchPosts(user.id)

  @data.welcome = "Hello, " + user.name
  @data.postCount = posts.length
</span><span class="token template-punctuation string">`</span></span><span class="token punctuation">;</span>

<span class="token keyword">const</span> result <span class="token operator">=</span> <span class="token keyword">await</span> env<span class="token punctuation">.</span><span class="token function">renderScriptString</span><span class="token punctuation">(</span>script<span class="token punctuation">,</span> context<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token comment">// { welcome: 'Hello, Alice', postCount: 42 }</span></code></pre>
<p><strong>‚ö†Ô∏è Heads up!</strong> Cascada is a new project. You might run into bugs, and the documentation is catching up with the code. Your feedback and contributions are welcome as we build the future of asynchronous programming.</p>
<hr>
<h2>Learn More</h2>
<p>üë®‚Äçüç≥ <a href="https://geleto.github.io/posts/cascada-kitchen-chef/">The Kitchen Chef's Guide to Concurrent Programming with Cascada</a> - Understand how Cascada works through a restaurant analogy - no technical jargon, just cooks, ingredients, and a brilliant manager who makes parallel execution feel as natural as following a recipe</p>
<p>üìñ <a href="https://github.com/geleto/cascada/blob/master/docs/cascada/script.md">Cascada Script Documentation</a></p>
<p>üß© <a href="https://github.com/geleto/cascada">Cascada GitHub Repository</a></p>
<p>ü§ñ <a href="https://github.com/geleto/casai">Casai</a> - an AI orchestration library built on Cascada for effortless, parallel agent and workflow design.</p>
<hr>
<h2>Final Thoughts</h2>
<p>If you've ever fought through promise hell, callback pyramids, or race conditions, Cascada might just feel like magic.</p>
<p>It turns async programming from a juggling act into a walk in the park. You write code like a human - simple, sequential, intuitive - and let the runtime handle the parallelism and safety behind the scenes.</p>
<p>The future of async programming isn't about getting better at promises. It's about not having to think about them at all.</p>

    ]]>
      </content>
    </entry>
  
    
    <entry>
      <title>semachunk: Minimal Semantic Chunker for RAG</title>
      <link href="https://geleto.github.io/posts/semachunk-semantic-chunking/"/>
      <updated>2025-11-27T00:00:00Z</updated>
      <id>https://geleto.github.io/posts/semachunk-semantic-chunking/</id>
      <content type="html">
        <![CDATA[
      <p>When building the <a href="https://github.com/geleto/casai-examples">examples</a> for <a href="https://github.com/geleto/casai">Casai</a> - my AI workflow framework - I wanted to include a proper RAG (Retrieval-Augmented Generation) example. Not a toy demo, but something that showed semantic chunking, vector search, and agentic filtering working together.</p>
<p>I hit a wall almost immediately: I couldn't find a TypeScript semantic chunker that fit my needs.</p>
<h2>The Problem</h2>
<p>Most semantic chunking libraries fall into one of three camps:</p>
<ol>
<li>
<p><strong>Heavy solutions</strong> that bundle their own embedding models, require downloads, spin up services, come with big frameworks and pull in massive dependencies</p>
</li>
<li>
<p><strong>Not API/provider agnostic</strong>  -  tightly coupled to a specific embedding provider or API, instead of letting you plug in your own embedding API of choice</p>
</li>
<li>
<p><strong>Fixed-size chunkers</strong> that just split on character count or tokens - missing the whole point of <em>semantic</em> chunking</p>
</li>
<li>
<p><strong>No batch embedding</strong>  -  they merge chunks sequentially, firing off a new embedding request after every single merge. Fine for local models, but hostile to API rate limits.</p>
</li>
</ol>
<p>I needed something different. For an examples repo, I wanted:</p>
<ul>
<li><strong>Zero infrastructure</strong>  -  no containers, no model downloads, no services</li>
<li><strong>Model agnostic</strong>  -  let users plug in whatever embedding provider they're already using (OpenAI, Anthropic, Google, local models, whatever)</li>
<li><strong>Batch-friendly</strong>  -  work <em>with</em> API rate limits, not against them</li>
<li><strong>Simple API</strong>  -  a single function call, not a framework</li>
</ul>
<h2>The Solution: semachunk</h2>
<p>I used <a href="https://github.com/jparkerweb/semantic-chunking">semantic-chunking</a> as a base - it has solid chunking implementation - and built <a href="https://github.com/geleto/semachunk">semachunk</a>, wrapping it in a callback-based API, and replacing the core merging algorithm with a new one that's optimized for batch embeddings, and is both more efficient and should produce better results (instead of merging linearly, it scores all adjacent chunk pairs, selects the best candidates, merges them in one pass, then batch re-embeds).</p>
<pre class="language-typescript"><code class="language-typescript"><span class="token keyword">import</span> <span class="token punctuation">{</span> chunkText <span class="token punctuation">}</span> <span class="token keyword">from</span> <span class="token string">'semachunk'</span><span class="token punctuation">;</span>

<span class="token comment">// Chunk the document semantically</span>
<span class="token keyword">const</span> chunks <span class="token operator">=</span> <span class="token keyword">await</span> <span class="token function">chunkText</span><span class="token punctuation">(</span>document<span class="token punctuation">,</span> <span class="token keyword">async</span> <span class="token punctuation">(</span>texts<span class="token punctuation">)</span> <span class="token operator">=></span> <span class="token punctuation">{</span>
    <span class="token comment">// Your embedding logic - any provider, any model</span>
    <span class="token keyword">const</span> response <span class="token operator">=</span> <span class="token keyword">await</span> openai<span class="token punctuation">.</span>embeddings<span class="token punctuation">.</span><span class="token function">create</span><span class="token punctuation">(</span><span class="token punctuation">{</span>
        input<span class="token operator">:</span> texts<span class="token punctuation">,</span>
        model<span class="token operator">:</span> <span class="token string">"text-embedding-3-small"</span>
    <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">return</span> response<span class="token punctuation">.</span>data<span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>d <span class="token operator">=></span> d<span class="token punctuation">.</span>embedding<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span><span class="token punctuation">,</span> <span class="token punctuation">{</span>
    maxChunkSize<span class="token operator">:</span> <span class="token number">500</span><span class="token punctuation">,</span>
    similarityThreshold<span class="token operator">:</span> <span class="token number">0.5</span><span class="token punctuation">,</span>
    returnEmbedding<span class="token operator">:</span> <span class="token boolean">true</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token comment">// Store in vector index</span>
<span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">const</span> chunk <span class="token keyword">of</span> chunks<span class="token punctuation">)</span> <span class="token punctuation">{</span>
    <span class="token keyword">await</span> index<span class="token punctuation">.</span><span class="token function">upsertItem</span><span class="token punctuation">(</span><span class="token punctuation">{</span>
        vector<span class="token operator">:</span> chunk<span class="token punctuation">.</span>embedding<span class="token punctuation">,</span>
        metadata<span class="token operator">:</span> <span class="token punctuation">{</span> text<span class="token operator">:</span> chunk<span class="token punctuation">.</span>text <span class="token punctuation">}</span>
    <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span></code></pre>
<p>That's it. You provide the text and an embedding function, you get back semantically coherent chunks ready for your vector store. No configuration files, no model downloads, no Docker containers.</p>
<h2>A New Algorithm</h2>
<p>The original semantic-chunking library didn't support batch embedding, and its merging algorithm wasn't structured in a way that made adding it straightforward - it merges chunks linearly, one pair at a time. So I wrote a new algorithm designed from the ground up to work in batches:</p>
<ol>
<li>Score <em>all</em> adjacent chunk pairs by similarity</li>
<li>Select the best merge candidates (configurable percentage)</li>
<li>Merge them in one pass</li>
<li>Batch re-embed all affected chunks</li>
<li>Repeat until no good merges remain</li>
</ol>
<p>This should produce better results (by considering all candidates globally rather than just the next pair) and drastically reduces API calls. Instead of potentially hundreds of single-embedding requests, you get a handful of batch requests.</p>
<p>The chunks preserve semantic coherence - a paragraph about a specific topic stays together rather than getting split mid-thought. This matters when you're retrieving context for an LLM; fragmented chunks lead to fragmented understanding.</p>
<h2>Trade-offs</h2>
<p>To be transparent: I haven't battle-tested this extensively. I built it for the Casai examples, and it works well for that use case. If you're processing millions of documents in production, you'll want to do your own evaluation.</p>
<p>The library is intentionally minimal. Just the core semantic chunking algorithm with a clean API.</p>
<h2>Try It</h2>
<p>Install directly:</p>
<pre class="language-bash"><code class="language-bash"><span class="token function">npm</span> <span class="token function">install</span> semachunk</code></pre>
<p>Or see it in action with the RAG example in <a href="https://github.com/geleto/casai-examples">casai-examples</a>:</p>
<pre class="language-bash"><code class="language-bash"><span class="token function">git</span> clone https://github.com/geleto/casai-examples.git
<span class="token builtin class-name">cd</span> casai-examples
<span class="token function">npm</span> <span class="token function">install</span>
<span class="token function">npm</span> run example <span class="token number">14</span></code></pre>
<p>GitHub: <a href="https://github.com/geleto/semachunk">github.com/geleto/semachunk</a></p>
<p>If you're building RAG pipelines and want semantic chunking without the infrastructure overhead, give it a try. Feedback and contributions welcome.</p>
<p><em>semachunk is derived from <a href="https://github.com/jparkerweb/semantic-chunking">semantic-chunking</a> by jparkerweb. Check out the original if you need local model support.</em></p>

    ]]>
      </content>
    </entry>
  
</feed>